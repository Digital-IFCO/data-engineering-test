{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f41592e-b5a1-4e55-bad8-d1cddd6384b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81532f93-5b7b-4407-8765-4185b9088378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker\tUntitled.ipynb\tdata  notebooks  src  streamlit_app  tests\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401da59c-9f72-4aff-8d6a-fbc6ea0247da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>date</th>\n",
       "      <th>company_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>crate_type</th>\n",
       "      <th>contact_data</th>\n",
       "      <th>salesowners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f47ac10b-58cc-4372-a567-0e02b2c3d479</td>\n",
       "      <td>29.01.22</td>\n",
       "      <td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td>\n",
       "      <td>Fresh Fruits Co</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>[{ \"contact_name\":\"Curtis\", \"contact_surname\":...</td>\n",
       "      <td>Leonard Cohen, Luke Skywalker, Ammy Winehouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f47ac10b-58cc-4372-a567-0e02b2c3d480</td>\n",
       "      <td>21.02.22</td>\n",
       "      <td>0f05a8f1-2bdf-4be7-8c82-4c9b58f04898</td>\n",
       "      <td>Veggies Inc</td>\n",
       "      <td>Wood</td>\n",
       "      <td>[{ \"contact_name\":\"Maria\", \"contact_surname\":\"...</td>\n",
       "      <td>Luke Skywalker, David Goliat, Leon Leonov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f47ac10b-58cc-4372-a567-0e02b2c3d481</td>\n",
       "      <td>03.04.22</td>\n",
       "      <td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td>\n",
       "      <td>Fresh Fruits c.o</td>\n",
       "      <td>Metal</td>\n",
       "      <td>[{ \"contact_name\":\"Para\", \"contact_surname\":\"C...</td>\n",
       "      <td>Luke Skywalker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f47ac10b-58cc-4372-a567-0e02b2c3d482</td>\n",
       "      <td>14.07.21</td>\n",
       "      <td>1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6</td>\n",
       "      <td>Seafood Supplier</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>David Goliat, Leonard Cohen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f47ac10b-58cc-4372-a567-0e02b2c3d483</td>\n",
       "      <td>23.10.22</td>\n",
       "      <td>34538e39-cd2e-4641-8d24-3c94146e6f16</td>\n",
       "      <td>Meat Packers Ltd</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chris Pratt, David Henderson, Marianov Merschi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               order_id      date  \\\n",
       "0  f47ac10b-58cc-4372-a567-0e02b2c3d479  29.01.22   \n",
       "1  f47ac10b-58cc-4372-a567-0e02b2c3d480  21.02.22   \n",
       "2  f47ac10b-58cc-4372-a567-0e02b2c3d481  03.04.22   \n",
       "3  f47ac10b-58cc-4372-a567-0e02b2c3d482  14.07.21   \n",
       "4  f47ac10b-58cc-4372-a567-0e02b2c3d483  23.10.22   \n",
       "\n",
       "                             company_id      company_name crate_type  \\\n",
       "0  1e2b47e6-499e-41c6-91d3-09d12dddfbbd   Fresh Fruits Co    Plastic   \n",
       "1  0f05a8f1-2bdf-4be7-8c82-4c9b58f04898       Veggies Inc       Wood   \n",
       "2  1e2b47e6-499e-41c6-91d3-09d12dddfbbd  Fresh Fruits c.o      Metal   \n",
       "3  1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6  Seafood Supplier    Plastic   \n",
       "4  34538e39-cd2e-4641-8d24-3c94146e6f16  Meat Packers Ltd    Plastic   \n",
       "\n",
       "                                        contact_data  \\\n",
       "0  [{ \"contact_name\":\"Curtis\", \"contact_surname\":...   \n",
       "1  [{ \"contact_name\":\"Maria\", \"contact_surname\":\"...   \n",
       "2  [{ \"contact_name\":\"Para\", \"contact_surname\":\"C...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         salesowners  \n",
       "0      Leonard Cohen, Luke Skywalker, Ammy Winehouse  \n",
       "1          Luke Skywalker, David Goliat, Leon Leonov  \n",
       "2                                     Luke Skywalker  \n",
       "3                        David Goliat, Leonard Cohen  \n",
       "4  Chris Pratt, David Henderson, Marianov Merschi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv (\"data/orders.csv\", sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b51fd8e-28ff-42e1-bb33-fb102eb425bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849766 sha256=b3e0cd989181e8b60e2d9a35a5f1c73f3b91f5d9b01b7e45537b58cb8ab9cc10\n",
      "  Stored in directory: /home/mooney/.cache/pip/wheels/13/92/64/da92a3521323cc629fdf25dd56eb26938e08014c1b57ad3759\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a52b8d7-0489-42d0-904f-80eca8978845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fe5b65-1354-4615-99a6-5377c32e44cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrderIngestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      2\u001b[0m df_orders \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"OrderIngestion\").getOrCreate()\n",
    "df_orders = spark.createDataFrame(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
